{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7d5a82-0e35-45ea-9118-b4988bf1dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcf80d-e13b-4426-8aff-bb2c906561a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d0421d7-25bb-41b3-9835-4b3ac3bc79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "datafolder = '../../data/hateful_memes/'\n",
    "test = datafolder+'test_unseen.jsonl'\n",
    "dev = datafolder+'dev_unseen.jsonl'\n",
    "# Load the data from the JSON file\n",
    "df_dev = pd.read_json(dev, lines = True)\n",
    "df_test = pd.read_json(test, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2641a-0357-4174-8ec2-44536d6fc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = df_dev.apply(preprocess_row, axis=1)\n",
    "df_test = df_test.apply(preprocess_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ddad25-6f3b-418e-b25f-cfa13843f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the NRC emotion lexicon into a dictionary with emotion words and corresponding associations\n",
    "lexicon = '../../data/hateful_memes/nrc-lexicon-en.txt' # path to the NRC emotion lexicon\n",
    "emotions = {}\n",
    "for line in open(lexicon).read().split('\\n'):\t\n",
    "    emotion_word = line.split('\\t')[0]\n",
    "    emotion = line.split('\\t')[1]\n",
    "    association = line.split('\\t')[2]\n",
    "    if association == \"1\":\n",
    "        if emotion_word in emotions:\n",
    "            emotions[emotion_word].append(emotion)\n",
    "        else:\n",
    "            emotions[emotion_word] = [emotion] \n",
    "\n",
    "fw_list = ['ADP', 'AUX', 'CCONJ', 'DET', 'NUM', 'PART', 'PRON', 'SCONJ'] # POS tags that correspond to function words\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-e2m-intent\")\n",
    "intent_model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-e2m-intent\")\n",
    "def get_intent(event):\n",
    "    input_text = \"%s </s>\" % event\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = intent_model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'])\n",
    "\n",
    "    return tokenizer.decode(output[0])[6:-4]\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "def senti(text):\n",
    "    output = sentiment_analysis(text)[0]\n",
    "    if output['label'] == 'POSITIVE':\n",
    "        return output['score']\n",
    "    else:\n",
    "        return 1 - output['score']\n",
    "    \n",
    "def get_feats_en(upos, lemmas, text):\t\n",
    "    pos_fw_emo = []\n",
    "    count = 0\n",
    "    emotion_associations = []\n",
    "    sentiment_score = senti(text)\n",
    "    intent = get_intent(text)\n",
    "    for i, lemma in enumerate(lemmas.split()):\n",
    "        if lemma.lower() in emotions:\n",
    "            pos_fw_emo.append(lemma)\n",
    "            count += 1\n",
    "            emotion_associations.append(emotions[lemma.lower()])     \n",
    "        else:\n",
    "            if upos.split()[i] in fw_list:\n",
    "                pos_fw_emo.append(lemma)\n",
    "            else:\n",
    "                pos_fw_emo.append(upos.split()[i])\n",
    "    emotion_associations = [emo for sublist in emotion_associations for emo in sublist]\n",
    "    return pd.Series([' '.join(pos_fw_emo), count, ' '.join(emotion_associations), sentiment_score, intent])\n",
    "\n",
    "df_dev[['pos_fw_emo', 'count', 'emotion_associations', 'sentiment_score', 'intent']] = df_dev.apply(lambda x: get_feats_en(x['upos'], x['lemmas'], x['text']), axis=1) \n",
    "df_test[['pos_fw_emo', 'count', 'emotion_associations', 'sentiment_score', 'intent']] = df_test.apply(lambda x: get_feats_en(x['upos'], x['lemmas'], x['text']), axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783f89a-143a-414d-a220-a4ae6c184fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.to_csv(datafolder+'dev_unseen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e893c-c45a-4656-b4b6-383fc2b90e4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c64d122-0279-40e4-b361-a453420716b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = '../../data/hateful_memes/'\n",
    "train1 = datafolder+'train_with_features.csv'\n",
    "train2 = datafolder+'test_with_features.csv'\n",
    "train3 = datafolder+'dev_with_features.csv'\n",
    "df_train1 = pd.read_csv(train1, skip_blank_lines=False)\n",
    "df_train2 = pd.read_csv(train2, skip_blank_lines=False)\n",
    "df_train3 = pd.read_csv(train3, skip_blank_lines=False)\n",
    "df_train = pd.concat([df_train1, df_train2, df_train3], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc4dbed6-6c83-4eae-879c-db72cc7c0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "datafolder = '../../data/hateful_memes/'\n",
    "test = datafolder+'test_unseen_with_features.csv'\n",
    "dev = datafolder+'dev_unseen_with_features.csv'\n",
    "df_dev = pd.read_csv(dev, keep_default_na=False)\n",
    "df_test = pd.read_csv(test, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dcb05c-416c-47a9-b9dd-69b1d7f12241",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# BERTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b50b88-95b7-43e1-b2cc-a25c8de961e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## HateBert wordembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6543c279-fcb1-48b0-a070-6c7993a90bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Hate-speech-CNERG/dehatebert-mono-english were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
    "model = AutoModel.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\").to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# get word embeddings of the sentences in the the text column of text\n",
    "train_vectors = get_vectors(df_train.text.to_list(), tokenizer, model)\n",
    "dev_vectors = get_vectors(df_dev.text.to_list(), tokenizer, model)\n",
    "test_vectors = get_vectors(df_test.text.to_list(), tokenizer, model)\n",
    "Y_train = df_train.label.values\n",
    "Y_dev = df_dev.label.values\n",
    "Y_test = df_test.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405b8909-255f-4dfd-a21a-df683c367a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf_svc = SVC(kernel='linear', max_iter=1000000, C=10) # parameter C was selected based on grid search\n",
    "clf_svc.fit(train_vectors, Y_train)\n",
    "\n",
    "df_dev['hatebert_vectors'] = clf_svc.predict(dev_vectors)\n",
    "df_test['hatebert_vectors'] = clf_svc.predict(test_vectors)\n",
    "\n",
    "df_dev.to_csv(datafolder+'dev_unseen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908f3d4-5dc8-4305-a898-b773ec7396d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## HateBert Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce5cd4c-5ee2-4d0c-b925-87785bc162a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline(\"text-classification\", model=\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
    "\n",
    "df_dev['hatebert_direct'] = df_dev.text.apply(lambda x: 0 if model(x)[0]['label'] == 'NON_HATE' else 1)\n",
    "df_test['hatebert_direct'] = df_test.text.apply(lambda x: 0 if model(x)[0]['label'] == 'NON_HATE' else 1)\n",
    "\n",
    "df_dev.to_csv(datafolder+'dev_unseen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5856b21-0ff6-4dc1-afed-fda1a22c4574",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## FineTune Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736ece24-7f0c-49ef-83a6-f4976749fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a83e00-a77a-4533-836e-756610cffb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1 with average training loss of 0.6018809530491265.\n",
      "Finished epoch 2 with average training loss of 0.5053156623349022.\n",
      "Finished epoch 3 with average training loss of 0.4280216864789256.\n",
      "Finished epoch 4 with average training loss of 0.3582127199481471.\n",
      "Finished epoch 5 with average training loss of 0.312639360753492.\n",
      "Finished epoch 6 with average training loss of 0.2812319396022029.\n",
      "Finished epoch 7 with average training loss of 0.2606272713873333.\n",
      "Finished epoch 8 with average training loss of 0.24544338860546058.\n",
      "Finished epoch 9 with average training loss of 0.23105390569843803.\n",
      "Finished epoch 10 with average training loss of 0.21971434107222876.\n"
     ]
    }
   ],
   "source": [
    "# Set up GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "tokenizer, model = fine_tune(df_train, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905bf330-eb82-4fce-b55a-a521baa91d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['bert_base_cased_finetuned'] = predict_from_fine_tuned(df_dev, tokenizer, model)\n",
    "df_test['bert_base_cased_finetuned'] = predict_from_fine_tuned(df_test, tokenizer, model)\n",
    "df_dev.to_csv(datafolder+'dev_unseen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3916e3dd-5b82-4bca-94ab-58bbd9cc63fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b2e8e-46ee-474f-ad03-d20fe64dd485",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Baseline: BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10bc0349-ecbe-493c-8250-294a69c19470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a0d174-e812-4dfc-8d29-ec78ac7d2f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(1, 1)) # to build 1n-grams from the word ==> BoW\n",
    "                \n",
    "X_train = vectorizer.fit_transform(df_train.tokens)\n",
    "X_dev = vectorizer.transform(df_dev.tokens) \n",
    "X_test = vectorizer.transform(df_test.tokens)\n",
    "\n",
    "Y_train = df_train.label.values\n",
    "Y_dev = df_dev.label.values\n",
    "Y_test = df_test.label.values\n",
    "\n",
    "clf_svc = LinearSVC(max_iter=1000000, C = 10,random_state =456)  # TODO: Hyperparms tuning\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "Y_pred = clf_svc.predict(X_dev)\n",
    "Y_pred2 = clf_svc.predict(X_test)\n",
    "\n",
    "df_dev['svm_BoW_baseline_linear_C10'] = Y_pred\n",
    "df_test['svm_BoW_baseline_linear_C10'] = Y_pred2\n",
    "df_dev.to_csv(datafolder+'dev_unseen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc15dd-32e6-47cd-8930-fdaf6d7e2890",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## character-n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4653f04-c031-4549-87cc-f399de9010dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(tokenizer=lambda x: list(x), analyzer='char', ngram_range=(1, 3)) # to build 1n-grams from the word ==> BoW\n",
    "\n",
    "X_train = vectorizer2.fit_transform(df_train.tokens)\n",
    "X_dev = vectorizer2.transform(df_dev.tokens) \n",
    "X_test = vectorizer2.transform(df_test.tokens)\n",
    "\n",
    "Y_train = df_train.label.values\n",
    "Y_dev = df_dev.label.values\n",
    "Y_test = df_test.label.values\n",
    "\n",
    "clf_svc = LinearSVC(max_iter=1000000, C = 10,random_state =456)\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "Y_pred = clf_svc.predict(X_dev)\n",
    "Y_pred2 = clf_svc.predict(X_test)\n",
    "\n",
    "df_dev['baseline_svm_char_kernelC10'] = Y_pred\n",
    "df_test['baseline_svm_char_kernelC10'] = Y_pred2\n",
    "df_dev.to_csv(datafolder+'dev_unseen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ac3ec-5259-46eb-8ecf-3c4da15461c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Advanced SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad14b8e1-8ac3-40c9-832b-749519b025aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6114f6bd-feae-4ad4-b628-5c98cba4bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(1, 3)) # to build n-grams (n=1-3) from the pos_fw_emo representation\n",
    "vectorizer2 = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(1, 1)) # unigrams of emotion associations\n",
    "vectorizer3 = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(1, 1)) # unigrams of tokens (BoW)\n",
    "vectorizer4 = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(4, 4)) # unigrams of intent (BoW)\n",
    "\n",
    "df_train = df_train.fillna('')\n",
    "# combine the features\n",
    "X_train = hstack((vectorizer1.fit_transform(df_train.pos_fw_emo), vectorizer2.fit_transform(df_train.emotion_associations), df_train[['count']].values, \\\n",
    "                df_train[['sentiment_score']].values, vectorizer3.fit_transform(df_train.tokens), vectorizer4.fit_transform(df_train.intent)), format='csr') \n",
    "\n",
    "X_dev = hstack((vectorizer1.transform(df_dev.pos_fw_emo), vectorizer2.transform(df_dev.emotion_associations), df_dev[['count']].values, \\\n",
    "                df_dev[['sentiment_score']].values, vectorizer3.transform(df_dev.tokens), vectorizer4.transform(df_dev.intent) ), format='csr') \n",
    "\n",
    "X_test = hstack((vectorizer1.transform(df_test.pos_fw_emo), vectorizer2.transform(df_test.emotion_associations), df_test[['count']].values, \\\n",
    "                df_test[['sentiment_score']].values, vectorizer3.transform(df_test.tokens), vectorizer4.transform(df_test.intent) ), format='csr') \n",
    "\n",
    "Y_train = df_train.label.values\n",
    "Y_dev = df_dev.label.values\n",
    "Y_test = df_test.label.values\n",
    "\n",
    "clf_svc = LinearSVC(max_iter=1000000, C = 10,random_state =456)\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "Y_pred = clf_svc.predict(X_dev)\n",
    "Y_pred2 = clf_svc.predict(X_test)\n",
    "\n",
    "df_dev['Advanced_svm_linear_100K_C10'] = Y_pred\n",
    "df_test['Advanced_svm_linear_100K_C10'] = Y_pred2\n",
    "df_dev.to_csv(datafolder+'dev_unseen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372015d-4ce7-44ec-8c3c-c77485f1bd01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9505e79-1e50-456a-920a-ec8e8139ca56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### create image tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef776b1e-c309-4e4c-9ee9-4f8b332fb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95762c54-d4fa-4a8e-a231-2900c9c937f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Hisha/.cache/huggingface/datasets/csv/default-98dec141641a1da8/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7171680e20094439b481a62efe7b751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Hisha/.cache/huggingface/datasets/csv/default-0c869af6f3d44c59/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e106ae2addcf4296b9562106c5f60cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ResNetModel: ['classifier.1.weight', 'classifier.1.bias']\n",
      "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ResNetModel: ['classifier.1.weight', 'classifier.1.bias']\n",
      "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# importing the data\n",
    "datafolder = '../../data/hateful_memes/'\n",
    "test = datafolder+'test_unseen_with_features.csv'\n",
    "dev = datafolder+'dev_unseen_with_features.csv'\n",
    "\n",
    "dev_data = load_dataset('csv', data_files=dev, num_proc=8).cast_column(\"img\", Image(decode=False))\n",
    "test_data = load_dataset('csv', data_files=test, num_proc=8).cast_column(\"img\", Image(decode=False))\n",
    "\n",
    "dev_img = get_image_vectors(dev_data, 'train', datafolder)\n",
    "test_img = get_image_vectors(test_data, 'train', datafolder)\n",
    "torch.save(dev_img, 'dev_unseen_img_tensors.pt')\n",
    "torch.save(test_img, 'test_unseen_img_tensors.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3666c1-86b6-44d4-9180-f757d67e0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have the following \n",
    "train_img1 = torch.stack(torch.load('train_img_tensors.pt'))\n",
    "train_img2 = torch.stack(torch.load('dev_img_tensors.pt'))\n",
    "train_img3 = torch.stack(torch.load('test_img_tensors.pt'))\n",
    "# we combine them into one big training set\n",
    "# train_img = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87b35590-0eae-4acc-9307-3e7f7df16432",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = torch.cat((train_img1,train_img2, train_img3), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e117368d-a82b-49c7-9322-e77204d43b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = df_train['label']\n",
    "Y_dev = np.asarray(dev_data['train']['label'])\n",
    "Y_test = np.asarray(test_data['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6ac157b-9e7c-4141-bef2-544d3cc1bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = [np.array(x.cpu()).flatten() for x in train_img]\n",
    "dev_X = [np.array(x.cpu()).flatten() for x in dev_img]\n",
    "test_X = [np.array(x.cpu()).flatten() for x in test_img]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82321b9-734a-4c62-8a69-2a3d20cfc50e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d129a16c-e484-4dab-aba2-7329e08dc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC(kernel='rbf', random_state =456)\n",
    "clf_svc.fit(train_X, Y_train)\n",
    "Y_pred = clf_svc.predict(dev_X)\n",
    "Y_pred2 = clf_svc.predict(test_X)\n",
    "df_dev['ResNet_svm_rbf_kernel'] = Y_pred\n",
    "df_test['ResNet_svm_rbf_kernel'] = Y_pred2\n",
    "\n",
    "df_dev.to_csv(datafolder+'dev_unseen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1337ee23-ba60-4fa5-b3b4-f6ce85e5599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc = SVC(kernel='linear', C=10, random_state =456)\n",
    "clf_svc.fit(train_X, Y_train)\n",
    "Y_pred = clf_svc.predict(dev_X)\n",
    "Y_pred2 = clf_svc.predict(test_X)\n",
    "\n",
    "df_dev['ResNet_svm_linear_kernelC10'] = Y_pred\n",
    "df_test['ResNet_svm_linear_kernelC10'] = Y_pred2\n",
    "\n",
    "df_dev.to_csv(datafolder+'dev_unseen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0358cd-d2c0-4338-b0a3-02f285444e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f716f7-c5bf-4552-8ded-2e80b0fd3aef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43b32c42-ff8a-4f5f-8df5-89be6d8ed484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1f26ff2-faf6-4e89-a8e4-bbd7b3710920",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = '../../data/hateful_memes/'\n",
    "train1 = datafolder+'test_with_features.csv'\n",
    "train2 = datafolder+'dev_with_features.csv'\n",
    "df_train1 = pd.read_csv(train1, skip_blank_lines=False)\n",
    "df_train2 = pd.read_csv(train2, skip_blank_lines=False)\n",
    "df_train = pd.concat([df_train1, df_train2], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f3f056d-7d72-41f7-9f5b-12d1011dba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "datafolder = '../../data/hateful_memes/'\n",
    "test = datafolder+'test_unseen_with_features.csv'\n",
    "dev = datafolder+'dev_unseen_with_features.csv'\n",
    "df_dev = pd.read_csv(dev, keep_default_na=False)\n",
    "df_test = pd.read_csv(test, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed98eb3a-36c6-4ce7-9a40-1c78649041b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['hatebert_vectors', 'hatebert_direct', 'bert_base_cased_finetuned',\n",
    "          'svm_BoW_baseline_linear_C10', 'baseline_svm_char_kernelC10', 'Advanced_svm_linear_100K_C10',\n",
    "         'ResNet_svm_rbf_kernel', 'ResNet_svm_linear_kernelC10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b439f9e3-fb91-4040-97cf-caa9ee2f95b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GradientBoostingClassifier()\n",
    "# train the simple classifier on the stacked features\n",
    "classifier.fit(X = df_train[models], y = df_train['label'])\n",
    "\n",
    "# make predictions on the test set\n",
    "y_dev = classifier.predict(df_dev[models])\n",
    "y_test = classifier.predict(df_test[models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3aa9959e-e878-4c28-b5c6-c1580215c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['GBensemble'] = y_dev\n",
    "df_test['GBensemble'] = y_test\n",
    "\n",
    "df_dev.to_csv(datafolder+'dev_unseen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1487b9-feaf-4488-961a-9e3cde3f2f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
