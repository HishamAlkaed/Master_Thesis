{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cade9e6f-d31d-4459-a76d-52106f1d9f57",
   "metadata": {},
   "source": [
    "# This notebook is for preprocessing the data:\n",
    "The follwoing has been done here:\n",
    "    \n",
    "    1- Data has been transfromed from json to csv format\n",
    "    2- multiple features have been added/engineered:\n",
    "        a- tokens: tokenzied text by spacy en_core_web_sm (separated by space): str\n",
    "        b- lemmas: lemmatized text by spacy en_core_web_sm (separated by space): str\n",
    "        c- upos: Universal PoS by spacy en_core_web_sm (separated by space): str\n",
    "        d- pos_fw_emo: representation of the text through POS tags, function words,\n",
    "           and emotion words (from this representation n-grams (n=1-3) are built)\n",
    "        e- count: number of emotion words in a text (int)\n",
    "        f- emotion_associations: emotion associations from the NRC emotion lexicon \n",
    "        g- Sentiment score: using siebert/sentiment-roberta-large-english from huggingface \n",
    "           we retrieve the sentiment score of the whole sentence\n",
    "        h- Intent: using mrm8488/t5-base-finetuned-e2m-intent we retrieve the intent of the sentence \n",
    "    3- This resulted in a dataset containing the following information\n",
    "\n",
    "        --------------------------------------------------------\n",
    "        id\t42953\n",
    "        img\timg/42953.png\n",
    "        label\t0\n",
    "        text\tits their character not their color that matters\n",
    "        --------------------------------------------------------\n",
    "        tokens\tits their character not their color that matters\n",
    "        lemmas\tits their character not their color that matter\n",
    "        upos\tPRON PRON NOUN PART PRON NOUN PRON VERB\n",
    "        pos_fw_emo\tits their NOUN not their NOUN that VERB\n",
    "        count\t0\n",
    "        emotion_associations\t\n",
    "        sentiment_score\t0.997626\n",
    "        intent\tto be admired\n",
    "        --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b2f752-8dc4-4f9c-bd84-58c363fb5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from scipy.sparse import hstack\n",
    "import spacy\n",
    "\n",
    "from transformers import pipeline, AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d326e1a0-647e-4812-952c-73e898face7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "datafolder = '../../data/hateful_memes/'\n",
    "train = datafolder+'train.jsonl'\n",
    "test = datafolder+'test_seen.jsonl'\n",
    "dev = datafolder+'dev_seen.jsonl'\n",
    "# Load the data from the JSON file\n",
    "df_train = pd.read_json(train, lines = True)\n",
    "df_dev = pd.read_json(dev, lines = True)\n",
    "df_test = pd.read_json(test, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4242243a-b9d7-4862-be3f-dee35755f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "datafolder = '../../data/hateful_memes/'\n",
    "test = datafolder+'test_unseen.jsonl'\n",
    "dev = datafolder+'dev_unseen.jsonl'\n",
    "# Load the data from the JSON file\n",
    "df_dev = pd.read_json(dev, lines = True)\n",
    "df_test = pd.read_json(test, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3f2f91-6753-446b-8f62-46ecf38bffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_row(row):\n",
    "    text = row['text']\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        lemma = token.lemma_\n",
    "        tokens.append((token.text, lemma, pos))\n",
    "    row['tokens'] = \" \".join([t[0] for t in tokens])\n",
    "    row['lemmas'] = \" \".join([t[1] for t in tokens])\n",
    "    row['upos'] = \" \".join([t[2] for t in tokens])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a735c80f-fc75-443b-869f-c987a333bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train.apply(preprocess_row, axis=1)\n",
    "df_dev = df_dev.apply(preprocess_row, axis=1)\n",
    "df_test = df_test.apply(preprocess_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee8a42f-9bb2-4d68-bfda-83608cf0a585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smut', ['disgust', 'fear', 'negative']),\n",
       " ('expletive', ['anger', 'negative']),\n",
       " ('greeting', ['positive', 'surprise']),\n",
       " ('measles', ['disgust', 'fear', 'negative', 'sadness']),\n",
       " ('proven', ['trust']),\n",
       " ('inept', ['anger', 'disgust', 'negative']),\n",
       " ('perverted', ['disgust', 'negative']),\n",
       " ('inconsequential', ['negative', 'sadness']),\n",
       " ('unfulfilled', ['anger', 'anticipation', 'negative', 'sadness', 'surprise']),\n",
       " ('tantalizing', ['anticipation', 'joy', 'negative', 'positive', 'surprise'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the NRC emotion lexicon into a dictionary with emotion words and corresponding associations\n",
    "lexicon = '../../data/hateful_memes/nrc-lexicon-en.txt' # path to the NRC emotion lexicon\n",
    "emotions = {}\n",
    "for line in open(lexicon).read().split('\\n'):\t\n",
    "    emotion_word = line.split('\\t')[0]\n",
    "    emotion = line.split('\\t')[1]\n",
    "    association = line.split('\\t')[2]\n",
    "    if association == \"1\":\n",
    "        if emotion_word in emotions:\n",
    "            emotions[emotion_word].append(emotion)\n",
    "        else:\n",
    "            emotions[emotion_word] = [emotion] \n",
    "\n",
    "list(emotions.items())[:10] # print first 3 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc68edf-4a64-48a6-abfa-f0ae077d57ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1295: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# - pos_fw_emo = representation of the text through POS tags, function words, and emotion words (from this representation n-grams (n=1-3) are built, see vectorize below)\n",
    "# - count = number of emotion words in a text\n",
    "# - emotion_associations = emotion associations from the NRC emotion lexicon\n",
    "# - Sentiment score = using siebert/sentiment-roberta-large-english from huggingface we retrieve the sentiment score of the whole sentence\n",
    "# - Intent = using mrm8488/t5-base-finetuned-e2m-intent we retrieve the intent of the sentence \n",
    "\n",
    "fw_list = ['ADP', 'AUX', 'CCONJ', 'DET', 'NUM', 'PART', 'PRON', 'SCONJ'] # POS tags that correspond to function words\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-e2m-intent\")\n",
    "intent_model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-e2m-intent\")\n",
    "def get_intent(event):\n",
    "    input_text = \"%s </s>\" % event\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = intent_model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'])\n",
    "\n",
    "    return tokenizer.decode(output[0])[6:-4]\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "def senti(text):\n",
    "    output = sentiment_analysis(text)[0]\n",
    "    if output['label'] == 'POSITIVE':\n",
    "        return output['score']\n",
    "    else:\n",
    "        return 1 - output['score']\n",
    "    \n",
    "def get_feats_en(upos, lemmas, text):\t\n",
    "    pos_fw_emo = []\n",
    "    count = 0\n",
    "    emotion_associations = []\n",
    "    sentiment_score = senti(text)\n",
    "    intent = get_intent(text)\n",
    "    for i, lemma in enumerate(lemmas.split()):\n",
    "        if lemma.lower() in emotions:\n",
    "            pos_fw_emo.append(lemma)\n",
    "            count += 1\n",
    "            emotion_associations.append(emotions[lemma.lower()])     \n",
    "        else:\n",
    "            if upos.split()[i] in fw_list:\n",
    "                pos_fw_emo.append(lemma)\n",
    "            else:\n",
    "                pos_fw_emo.append(upos.split()[i])\n",
    "    emotion_associations = [emo for sublist in emotion_associations for emo in sublist]\n",
    "    return pd.Series([' '.join(pos_fw_emo), count, ' '.join(emotion_associations), sentiment_score, intent])\n",
    "\n",
    "# df_train[['pos_fw_emo', 'count', 'emotion_associations', 'sentiment_score', 'intent']] = df_train.apply(lambda x: get_feats_en(x['upos'], x['lemmas'], x['text']), axis=1) \n",
    "df_dev[['pos_fw_emo', 'count', 'emotion_associations', 'sentiment_score', 'intent']] = df_dev.apply(lambda x: get_feats_en(x['upos'], x['lemmas'], x['text']), axis=1) \n",
    "df_test[['pos_fw_emo', 'count', 'emotion_associations', 'sentiment_score', 'intent']] = df_test.apply(lambda x: get_feats_en(x['upos'], x['lemmas'], x['text']), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264101a-830d-4e71-bc53-505130582814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['sentiment_score'] = df_train.apply(lambda x: senti(x['text']), axis=1)\n",
    "# df_dev['sentiment_score'] = df_dev.apply(lambda x: senti(x['text']), axis=1)\n",
    "# df_test['sentiment_score'] = df_test.apply(lambda x: senti(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5bc8a5-7cb3-4fbb-a491-fd375792035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv(datafolder+'train_with_features.csv', index=False)\n",
    "df_dev.to_csv(datafolder+'dev_uneen_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_unseen_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893487f6-39b6-46fa-ad07-01345c703c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
