{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a353efd8-478c-4b20-b795-9c412e09768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# import torch\n",
    "\n",
    "import nltk, csv, collections\n",
    "# nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from scipy.sparse import hstack\n",
    "import spacy\n",
    "\n",
    "from transformers import pipeline, AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad6e1f5-e7e9-49bd-bee2-c3819e3916c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "datafolder = '../../data/hateful_memes/'\n",
    "train = datafolder+'train.jsonl'\n",
    "test = datafolder+'test_seen.jsonl'\n",
    "dev = datafolder+'dev_seen.jsonl'\n",
    "# Load the data from the JSON file\n",
    "df_train = pd.read_json(train, lines = True)\n",
    "df_dev = pd.read_json(dev, lines = True)\n",
    "df_test = pd.read_json(test, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b482e6-6820-4955-9996-3d4f39627555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_row(row):\n",
    "    text = row['text']\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        lemma = token.lemma_\n",
    "        tokens.append((token.text, lemma, pos))\n",
    "    row['tokens'] = \" \".join([t[0] for t in tokens])\n",
    "    row['lemmas'] = \" \".join([t[1] for t in tokens])\n",
    "    row['upos'] = \" \".join([t[2] for t in tokens])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9160ee45-fe0f-499a-b937-a7bb8eb90f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.apply(preprocess_row, axis=1)\n",
    "df_dev = df_dev.apply(preprocess_row, axis=1)\n",
    "df_test = df_test.apply(preprocess_row, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c03b6f-18c9-450a-bcac-ee14c8b061e8",
   "metadata": {},
   "source": [
    "# advanced SVM \n",
    "#### - pos_fw_emo = representation of the text through POS tags, function words, and emotion words (from this representation n-grams (n=1-3) are built, see vectorize below)\n",
    "#### - count = number of emotion words in a text\n",
    "#### - emotion_associations = emotion associations from the NRC emotion lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a1c573a-5299-4ef1-898b-6d54cfe2df15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smut', ['disgust', 'fear', 'negative']),\n",
       " ('expletive', ['anger', 'negative']),\n",
       " ('greeting', ['positive', 'surprise'])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the NRC emotion lexicon into a dictionary with emotion words and corresponding associations\n",
    "lexicon = '../../data/hateful_memes/nrc-lexicon-en.txt' # path to the NRC emotion lexicon\n",
    "emotions = {}\n",
    "for line in open(lexicon).read().split('\\n'):\t\n",
    "    emotion_word = line.split('\\t')[0]\n",
    "    emotion = line.split('\\t')[1]\n",
    "    association = line.split('\\t')[2]\n",
    "    if association == \"1\":\n",
    "        if emotion_word in emotions:\n",
    "            emotions[emotion_word].append(emotion)\n",
    "        else:\n",
    "            emotions[emotion_word] = [emotion] \n",
    "\n",
    "list(emotions.items())[:3] # print first 3 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53d9dbcf-ba11-4078-8a80-cbaeee4efdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# - pos_fw_emo = representation of the text through POS tags, function words, and emotion words (from this representation n-grams (n=1-3) are built, see vectorize below)\n",
    "# - count = number of emotion words in a text\n",
    "# - emotion_associations = emotion associations from the NRC emotion lexicon\n",
    "# - Sentiment score = using siebert/sentiment-roberta-large-english from huggingface we retrieve the sentiment score of the whole sentence\n",
    "# - Intent = using mrm8488/t5-base-finetuned-e2m-intent we retrieve the intent of the sentence \n",
    "\n",
    "fw_list = ['ADP', 'AUX', 'CCONJ', 'DET', 'NUM', 'PART', 'PRON', 'SCONJ'] # POS tags that correspond to function words\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-e2m-intent\")\n",
    "intent_model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-e2m-intent\")\n",
    "def get_intent(event):\n",
    "    input_text = \"%s </s>\" % event\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = intent_model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'])\n",
    "\n",
    "    return tokenizer.decode(output[0])[6:-4]\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "def senti(text):\n",
    "    output = sentiment_analysis(text)[0]\n",
    "    if output['label'] == 'POSITIVE':\n",
    "        return output['score']\n",
    "    else:\n",
    "        return 1 - output['score']\n",
    "    \n",
    "def get_feats_en(upos, lemmas, text):\t\n",
    "    pos_fw_emo = []\n",
    "    count = 0\n",
    "    emotion_associations = []\n",
    "    sentiment_score = senti(text)\n",
    "    intent = get_intent(text)\n",
    "    for i, lemma in enumerate(lemmas.split()):\t\t\n",
    "        if lemma.lower() in emotions:\n",
    "            pos_fw_emo.append(lemma)\n",
    "            count += 1\n",
    "            emotion_associations.append(emotions[lemma.lower()])     \n",
    "        else:\n",
    "            if upos.split()[i] in fw_list:\n",
    "                pos_fw_emo.append(lemma)\n",
    "            else:\n",
    "                pos_fw_emo.append(upos.split()[i])\n",
    "    emotion_associations = [emo for sublist in emotion_associations for emo in sublist]\n",
    "    return pd.Series([' '.join(pos_fw_emo), count, ' '.join(emotion_associations), sentiment_score, intent])\n",
    "\n",
    "df_train[['pos_fw_emo', 'count', 'emotion_associations', 'sentiment_score', 'intent']] = df_train.apply(lambda x: get_feats_en(x['upos'], x['lemmas'], x['text']), axis=1) \n",
    "df_dev[['pos_fw_emo', 'count', 'emotion_associations', 'sentiment_score', 'intent']] = df_dev.apply(lambda x: get_feats_en(x['upos'], x['lemmas'], x['text']), axis=1) \n",
    "df_test[['pos_fw_emo', 'count', 'emotion_associations', 'sentiment_score', 'intent']] = df_test.apply(lambda x: get_feats_en(x['upos'], x['lemmas'], x['text']), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1980732-4919-4745-910b-a83f7574a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['sentiment_score'] = df_train.apply(lambda x: senti(x['text']), axis=1)\n",
    "# df_dev['sentiment_score'] = df_dev.apply(lambda x: senti(x['text']), axis=1)\n",
    "df_test['sentiment_score'] = df_test.apply(lambda x: senti(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3c7ec71b-6237-4d4e-8791-dfb451c08888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(datafolder+'train_with_features.csv', index=False)\n",
    "df_dev.to_csv(datafolder+'dev_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9ce308-d0ed-4116-b6ba-9caa672e6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "datafolder = '../../data/hateful_memes/'\n",
    "train = datafolder+'train_with_features.csv'\n",
    "test = datafolder+'test_with_features.csv'\n",
    "dev = datafolder+'dev_with_features.csv'\n",
    "df_train = pd.read_csv(train, keep_default_na=False)\n",
    "df_dev = pd.read_csv(dev, keep_default_na=False)\n",
    "df_test = pd.read_csv(test, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f4f438-5528-4226-83e6-002a1cf26443",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(1, 3)) # to build n-grams (n=1-3) from the pos_fw_emo representation\n",
    "vectorizer2 = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(1, 1)) # unigrams of emotion associations\n",
    "vectorizer3 = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(1, 1)) # unigrams of tokens (BoW)\n",
    "vectorizer4 = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(1, 4)) # unigrams of intent (BoW)\n",
    "\n",
    "\n",
    "# combine the features\n",
    "X_train = hstack((vectorizer1.fit_transform(df_train.pos_fw_emo), vectorizer2.fit_transform(df_train.emotion_associations), df_train[['count']].values, \\\n",
    "                df_train[['sentiment_score']].values, vectorizer3.fit_transform(df_train.tokens), vectorizer4.fit_transform(df_train.intent)), format='csr') \n",
    "\n",
    "X_dev = hstack((vectorizer1.transform(df_dev.pos_fw_emo), vectorizer2.transform(df_dev.emotion_associations), df_dev[['count']].values, \\\n",
    "                df_dev[['sentiment_score']].values, vectorizer3.transform(df_dev.tokens), vectorizer4.transform(df_dev.intent) ), format='csr') \n",
    "\n",
    "X_test = hstack((vectorizer1.transform(df_test.pos_fw_emo), vectorizer2.transform(df_test.emotion_associations), df_test[['count']].values, \\\n",
    "                df_test[['sentiment_score']].values, vectorizer3.transform(df_test.tokens), vectorizer4.transform(df_test.intent) ), format='csr') \n",
    "\n",
    "Y_train = df_train.label.values\n",
    "Y_dev = df_dev.label.values\n",
    "Y_test = df_test.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ac4329-574c-4fa9-965d-902786d2015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_svc = LinearSVC(max_iter=100000) # parameter C was selected based on grid search\n",
    "clf_svc = LinearSVC(max_iter=100000, C = 10,random_state =456)\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "Y_pred = clf_svc.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ea1467-308c-46fc-8f69-5bdf15b4989c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.542501</td>\n",
       "      <td>0.533197</td>\n",
       "      <td>0.50764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall       F1\n",
       "0   0.542501  0.533197  0.50764"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(\n",
    "    [list(precision_recall_fscore_support(Y_dev, Y_pred, average='macro')[:3])],\n",
    "    columns=['precision', 'recall', 'F1'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f645e3a-461c-4c3e-b58b-329f7666dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred2 = clf_svc.predict(X_test)\n",
    "\n",
    "df_dev['Advanced_svm_linear_100K_C10'] = Y_pred\n",
    "df_test['Advanced_svm_linear_100K_C10'] = Y_pred2\n",
    "\n",
    "df_dev.to_csv(datafolder+'dev_with_features.csv', index=False)\n",
    "df_test.to_csv(datafolder+'test_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d403d0-b4dc-46eb-9e79-c368c7c901cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
